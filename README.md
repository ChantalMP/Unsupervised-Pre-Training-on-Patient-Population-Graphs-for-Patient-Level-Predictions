# Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions

This is the official code to the following arxiv paper:

["Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions"](https://arxiv.org/abs/2203.12616)

We published an extension of this paper, which can be found here:

["Unsupervised pre-training of graph transformers on patient population graphs"](https://arxiv.org/abs/2207.10603), [Code](https://github.com/ChantalMP/Unsupervised_pre-training_of_graph_transformers_on_patient_population_graphs)

## Graphormer Model

Our model code is based on the work of Ying et al. "Do Transformers Really Perform Badly for Graph Representation?" [1], published in the following repository [https://github.com/microsoft/Graphormer](https://github.com/microsoft/Graphormer)

[1] Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., Liu, T.Y.: Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems 34 (2021)

## Citation
Please cite this paper if you use the code:

```
@misc{https://doi.org/10.48550/arxiv.2203.12616,
  doi = {10.48550/ARXIV.2203.12616},
  url = {https://arxiv.org/abs/2203.12616},
  author = {Pellegrini, Chantal and Kazi, Anees and Navab, Nassir},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unsupervised Pre-Training on Patient Population Graphs for Patient-Level Predictions},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
```
